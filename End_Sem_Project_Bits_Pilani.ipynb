{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2pSMTrfvBDv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66b09Zp9vCvF"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBk7S8N_rYLH",
    "outputId": "63c020d7-329f-4a6b-a606-01de488d0ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
      "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.35.99)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.99 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.35.99)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.4)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.99->boto3) (2.2.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask) (3.21.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas dask duckdb xgboost scikit-learn shap matplotlib seaborn boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjHbGZPOupqm",
    "outputId": "4e03e4d4-8d1c-4a00-f92d-4792eaea366c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dIKwMYQvLsc"
   },
   "outputs": [],
   "source": [
    "# Install the necessary Dask package\n",
    "!pip install dask[dataframe] --quiet\n",
    "\n",
    "# Import necessary libraries\n",
    "import dask.dataframe as dd\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to recommend retention strategies\n",
    "def recommend_action(churn_probability):\n",
    "    \"\"\"Recommend actions based on churn probability.\"\"\"\n",
    "    if churn_probability > 0.75:\n",
    "        return \"High Risk: Offer a discount or special service.\"\n",
    "    elif 0.50 < churn_probability <= 0.75:\n",
    "        return \"Moderate Risk: Send a personalized email to re-engage.\"\n",
    "    else:\n",
    "        return \"Low Risk: No immediate action needed.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofu09Wvlvv0y",
    "outputId": "64b768e7-9c37-40e4-e13a-cfab9d333a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "KKUzJOKuwBtB",
    "outputId": "bf377b81-1531-411a-c057-61f295eb3384"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-5c1606e0-2b1a-46fc-9d1d-27f98dcb6ad8\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-5c1606e0-2b1a-46fc-9d1d-27f98dcb6ad8\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from google.colab import files\n",
    "import dask.dataframe as dd\n",
    "import duckdb\n",
    "import glob\n",
    "\n",
    "# Step 1: Upload files from local machine\n",
    "uploaded_files = files.upload()  # This will prompt you to select files to upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzVg8a8XxBWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-28CaQTTk_vP"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghcaaGnR2V_s"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Js0nSS32Wf5"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import duckdb\n",
    "import glob\n",
    "\n",
    "# Load all Parquet files using Dask\n",
    "parquet_files = glob.glob(\"train_*.parquet\")  # Adjust the path as needed\n",
    "print(f\"Total Parquet files loaded: {len(parquet_files)}\")  # Check the number of files loaded\n",
    "dask_df = dd.read_parquet(parquet_files)\n",
    "\n",
    "# Create the transaction amount feature as a proxy\n",
    "dask_df['transaction_amount'] = (\n",
    "    dask_df['bank_transfer_in_volume'] +\n",
    "    dask_df['bank_transfer_out_volume'] +\n",
    "    dask_df['crypto_in_volume'] +\n",
    "    dask_df['crypto_out_volume']\n",
    ")\n",
    "\n",
    "# Convert Dask DataFrame to a Pandas DataFrame\n",
    "# Note: Use `.compute()` only if the data fits in memory\n",
    "pandas_df = dask_df.compute()\n",
    "\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Create a DuckDB table from the Pandas DataFrame for SQL querying\n",
    "con.execute(\"CREATE TABLE bank_data AS SELECT * FROM pandas_df\")\n",
    "\n",
    "# Check unique values in the churn column\n",
    "churn_counts = pandas_df['churn_due_to_fraud'].value_counts()  # Adjust 'churn_due_to_fraud' if needed\n",
    "print(\"Churn Counts:\\n\", churn_counts)  # Print churn counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgh6HaBvlhE-"
   },
   "outputs": [],
   "source": [
    "len(dask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZeWumDimN97"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9F_xNTRmlUG"
   },
   "outputs": [],
   "source": [
    "con.execute(\"DESCRIBE bank_data\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouP_nt8Fmm3C"
   },
   "source": [
    "#**Step 2: Data Exploration and Time-Series Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPkKzVxUvIuA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM-wwxHPvS0m"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the SQL query uses the correct column names\n",
    "# Use the correct date column name in place of 'date'\n",
    "time_series_data = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        YEAR(date) AS year,\n",
    "        SUM(transaction_amount) AS total_transaction\n",
    "    FROM bank_data\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Check if time_series_data is empty\n",
    "if time_series_data.empty:\n",
    "    print(\"No data found for the time series analysis.\")\n",
    "else:\n",
    "    # Plotting the time series with enhanced visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_series_data['year'], time_series_data['total_transaction'], marker='o', color='b')\n",
    "    plt.title('Total Transaction Amount Over Years', fontsize=16)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "    plt.ylabel('Total Transaction Amount', fontsize=14)\n",
    "    plt.xticks(time_series_data['year'], rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFTngXltvTSD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkDEoCjGmuNl"
   },
   "outputs": [],
   "source": [
    "# Run a SQL query to summarize transaction amounts by year\n",
    "time_series_data = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        YEAR(date) AS year,\n",
    "        SUM(bank_transfer_out_volume) AS total_transaction\n",
    "    FROM bank_data\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Plotting the time series with enhanced visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_series_data['year'], time_series_data['total_transaction'], marker='o', color='b')\n",
    "plt.title('Total Transaction Amount Over Years', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Total Transaction Amount', fontsize=14)\n",
    "plt.xticks(time_series_data['year'], rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt0rJ-EDm06J"
   },
   "outputs": [],
   "source": [
    "con.execute(\"DESCRIBE bank_data\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kazjf0ugn4gD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcx3DKdFo2Og"
   },
   "source": [
    "# **Step 3: Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3U_H_-Io3uz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLRZzubBrNuu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S1qpHTOrhrd"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Load and inspect your data\n",
    "# Assuming `dask_df` is already loaded\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "dask_df['date'] = dd.to_datetime(dask_df['date'])\n",
    "\n",
    "# Calculate recency\n",
    "current_date = pd.Timestamp.now()\n",
    "dask_df['recency'] = (current_date - dask_df['date']).dt.days\n",
    "\n",
    "\n",
    "# Group by 'customer_id' and aggregate\n",
    "features = dask_df.groupby('customer_id').agg({\n",
    "    'recency': 'min',  # Minimum recency\n",
    "    'transaction_amount': 'mean',  # Average transaction amount\n",
    "    'churn_due_to_fraud': 'max'  # Assuming this indicates churn\n",
    "}).reset_index()\n",
    "\n",
    "# Compute the result\n",
    "features = features.compute()\n",
    "\n",
    "# Preview the features\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiGCuLJipDE3"
   },
   "outputs": [],
   "source": [
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrKJVidhpuBq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBJ2Vun1tVNm"
   },
   "source": [
    "# **Step 4: Data Visualization for Feature Insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZF7pJDPtWQs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your DataFrame named pandas_df\n",
    "# Calculate skewness for each column\n",
    "skewness = pandas_df.skew().sort_values(ascending=False)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "skewness_df = pd.DataFrame({'Feature': skewness.index, 'Skewness': skewness.values})\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the skewness\n",
    "sns.barplot(x='Skewness', y='Feature', data=skewness_df, palette='viridis')\n",
    "plt.axvline(0, color='red', linestyle='--')  # Add a line at x=0 for reference\n",
    "plt.title('Skewness of Features', fontsize=16)\n",
    "plt.xlabel('Skewness', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "br_lX5XAUMkV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your DataFrame named pandas_df\n",
    "# Select only numeric columns\n",
    "numeric_df = pandas_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate skewness for each numeric column\n",
    "skewness = numeric_df.skew().sort_values(ascending=False)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "skewness_df = pd.DataFrame({'Feature': skewness.index, 'Skewness': skewness.values})\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the skewness\n",
    "sns.barplot(x='Skewness', y='Feature', data=skewness_df, palette='viridis')\n",
    "plt.axvline(0, color='red', linestyle='--')  # Add a line at x=0 for reference\n",
    "plt.title('Skewness of Numeric Features', fontsize=16)\n",
    "plt.xlabel('Skewness', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myr00Cc5UeQP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your DataFrame named pandas_df\n",
    "# Select only numeric columns\n",
    "numeric_df = pandas_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate skewness for each numeric column\n",
    "skewness = numeric_df.skew()\n",
    "\n",
    "# Set the figure size for the plots\n",
    "plt.figure(figsize=(15, 5 * len(skewness)))\n",
    "\n",
    "# Iterate through each numeric column and plot skewness\n",
    "for i, column in enumerate(numeric_df.columns):\n",
    "    plt.subplot(len(numeric_df.columns), 1, i + 1)  # Create a subplot for each feature\n",
    "    sns.histplot(numeric_df[column], kde=True, color='skyblue', bins=30)\n",
    "    plt.axvline(numeric_df[column].mean(), color='red', linestyle='--', label='Mean')\n",
    "    plt.axvline(numeric_df[column].median(), color='green', linestyle='--', label='Median')\n",
    "    plt.title(f'Distribution of {column} (Skewness: {skewness[column]:.2f})', fontsize=16)\n",
    "    plt.xlabel(column, fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrAosrq8Uevf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WrScox6t5YZ"
   },
   "outputs": [],
   "source": [
    "# Make sure features DataFrame has the correct columns\n",
    "# Check the columns in the features DataFrame\n",
    "print(features.columns)\n",
    "\n",
    "# Visualizing the distribution of churn\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=features, x='churn_due_to_fraud', palette='pastel')\n",
    "plt.title('Churn Distribution', fontsize=16)\n",
    "plt.xlabel('Churn', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'], rotation=0)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the average transaction amount by churn status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=features, x='churn_due_to_fraud', y='transaction_amount', palette='viridis')\n",
    "plt.title('Average Transaction Amount by Churn Status', fontsize=16)\n",
    "plt.xlabel('Churn', fontsize=14)\n",
    "plt.ylabel('Average Transaction Amount', fontsize=14)\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'], rotation=0)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MGqwv596oDg"
   },
   "outputs": [],
   "source": [
    "# Pie chart for churn distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "churn_counts = features['churn_due_to_fraud'].value_counts()\n",
    "plt.pie(churn_counts, labels=['Not Churned', 'Churned'], autopct='%1.3f%%', startangle=90, colors=['#4CAF50', '#FF5722'])\n",
    "plt.title('Churn Distribution', fontsize=16)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is circular\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxLf2BQJ7O9l"
   },
   "outputs": [],
   "source": [
    "# Box plot for transaction amount by churn status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=features, x='churn_due_to_fraud', y='transaction_amount', palette='Set2')\n",
    "plt.title('Transaction Amount Distribution by Churn Status', fontsize=16)\n",
    "plt.xlabel('Churn Status', fontsize=14)\n",
    "plt.ylabel('Transaction Amount', fontsize=14)\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'], rotation=0)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6dfRBWv-bsS"
   },
   "outputs": [],
   "source": [
    "# Violin plot for transaction amount by churn status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=features, x='churn_due_to_fraud', y='transaction_amount', palette='muted')\n",
    "plt.title('Transaction Amount Distribution by Churn Status', fontsize=16)\n",
    "plt.xlabel('Churn Status', fontsize=14)\n",
    "plt.ylabel('Transaction Amount', fontsize=14)\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'], rotation=0)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coponCet-lNM"
   },
   "outputs": [],
   "source": [
    "# Heatmap for correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation = features.corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW46jlEd-tjy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBHNxD3cAYzK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo-qme2eAZVx"
   },
   "source": [
    "#**Step 5: Prepare Data for Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1rCWDLqAc2x"
   },
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "X = features.drop(['customer_id', 'churn'], axis=1)\n",
    "y = features['churn']\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5jFdPcMAlrg"
   },
   "outputs": [],
   "source": [
    "# Check the columns in the features DataFrame to find the correct churn column name\n",
    "print(\"Features DataFrame Columns:\", features.columns.tolist())\n",
    "\n",
    "# Prepare features for modeling\n",
    "# Replace 'churn' with the correct column name if it's different\n",
    "X = features.drop(['customer_id', 'churn_due_to_fraud'], axis=1)  # Adjust 'churn_due_to_fraud' if needed\n",
    "y = features['churn_due_to_fraud']  # Adjust 'churn_due_to_fraud' if needed\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpER7l0rBL6v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFUyoHLxDk6p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQjSHsJLDlIq"
   },
   "source": [
    "#**Step 6: Train the XGBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii397EkXDrfs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU-yEhx0DzC_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMXbgLLYEdv3"
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the XGBoost model without use_label_encoder\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    model.fit(X_train, y_train)\n",
    "except AttributeError as e:\n",
    "    print(f\"Error during model fitting: {e}\")\n",
    "\n",
    "# Make predictions\n",
    "try:\n",
    "    y_pred = model.predict(X_test)\n",
    "except ValueError as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "\n",
    "# Evaluate the model\n",
    "try:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    # Handle UndefinedMetricWarning by using zero_division parameter\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "except ValueError as e:\n",
    "    print(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyl_RoMJFA6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joUgIERiFeZX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNsTGfD2FejN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6DdDdCnFevQ"
   },
   "source": [
    "#**Step 7: Explainable AI using SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsvK3xgQFj6T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9PJYJJeFn_d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbNmQg6gF4nG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaNm02rYGeT8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MC69U5vHMpm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDkEOdS0HqPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ERzWWeeJ7TN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RgCO8bfJ7Z5"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have the SHAP values for your test data\n",
    "# Create the explainer\n",
    "explainer = shap.Explainer(model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Check the shape of SHAP values\n",
    "print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "# Create a summary plot for all features\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.title('SHAP Summary Plot for All Features', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Generate SHAP dependence plots for important features\n",
    "for feature_name in X_test.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    shap.dependence_plot(feature_name, shap_values.values, X_test)\n",
    "    plt.title(f'SHAP Dependence Plot for {feature_name}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjh8Z3O0J7ic"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvug0FgmJ7sU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AihrY_KXRScv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCwdcfZjRS8H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtfyyVhyHqcZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xjuyxbvRgaO"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdhIvR59Rgvn"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure you have the SHAP values for your test data\n",
    "# Create the explainer\n",
    "explainer = shap.Explainer(model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Print the shape of SHAP values\n",
    "print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "# Create a summary plot for all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test, show=False)  # Default color map is used\n",
    "plt.title('SHAP Summary Plot for All Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate SHAP dependence plots for important features\n",
    "important_features = X_test.columns[:5]  # Change this to the top N features based on your analysis\n",
    "for feature_name in important_features:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    shap.dependence_plot(feature_name, shap_values.values, X_test, show=False)\n",
    "    plt.title(f'SHAP Dependence Plot for {feature_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Additional Waterfall Plot for an individual instance\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.plots.waterfall(shap_values[0])  # Change index for different instances\n",
    "plt.title('SHAP Waterfall Plot for First Instance', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq7RXy0iRm96"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KZgfJbhSQps"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cifBZ32YSf23"
   },
   "source": [
    "#**Step 8: Implement Retention Strategy Recommender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3t67HpESQz8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure that the index of X_test is reset to match with customer_id\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a DataFrame with predictions and recommendations\n",
    "predictions_df = pd.DataFrame({\n",
    "    'customer_id': X_test.index,  # Use the index or a specific customer ID column if available\n",
    "    'churn_probability': model.predict_proba(X_test)[:, 1]  # Get the probability of churn\n",
    "})\n",
    "\n",
    "# Apply the recommendation function to generate actions based on churn probability\n",
    "predictions_df['recommendation'] = predictions_df['churn_probability'].apply(recommend_action)\n",
    "\n",
    "# Output the predictions and recommendations\n",
    "print(predictions_df[['customer_id', 'churn_probability', 'recommendation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0Tg241wSQ9L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIG-AWACS3Sv"
   },
   "source": [
    "#**Step 9: Save the Predictions for Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga0HMrjsSjz-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory to save the predictions (optional)\n",
    "output_directory = 'output'  # You can specify any directory\n",
    "os.makedirs(output_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the path for the submission file\n",
    "submission_file_path = os.path.join(output_directory, 'sample_submission.csv')\n",
    "\n",
    "# Save the predictions for submission\n",
    "predictions_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {submission_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-TGzOrrTAwe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named `pandas_df` with columns: 'customer_id', 'transaction_date', and 'transaction_amount'\n",
    "# Make sure 'transaction_date' is in datetime format\n",
    "pandas_df['date'] = pd.to_datetime(pandas_df['date'])\n",
    "\n",
    "# Current date for recency calculation\n",
    "current_date = pd.to_datetime('today')\n",
    "\n",
    "# Calculate RFM\n",
    "rfm_df = pandas_df.groupby('customer_id').agg({\n",
    "    'date': lambda x: (current_date - x.max()).days,  # Recency\n",
    "    'customer_id': 'count',  # Frequency (count of transactions)\n",
    "    'transaction_amount': 'sum'  # Monetary (sum of transaction amounts)\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "rfm_df.columns = ['customer_id', 'recency', 'frequency', 'monetary']\n",
    "\n",
    "# Display RFM DataFrame\n",
    "print(rfm_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Na6QA7X-hsNe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuZp7d2Ahsg1"
   },
   "source": [
    "#**RFM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C8dbZBKYCrw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named `pandas_df` with columns: 'customer_id', 'transaction_date', and 'transaction_amount'\n",
    "# Make sure 'transaction_date' is in datetime format\n",
    "pandas_df['date'] = pd.to_datetime(pandas_df['date'])\n",
    "\n",
    "# Current date for recency calculation\n",
    "current_date = pd.to_datetime('today')\n",
    "\n",
    "# Calculate RFM\n",
    "rfm_df = pandas_df.groupby('customer_id').agg(\n",
    "    recency=('date', lambda x: (current_date - x.max()).days),  # Recency\n",
    "    frequency=('transaction_date', 'count'),  # Frequency (count of transactions)\n",
    "    monetary=('transaction_amount', 'sum')  # Monetary (sum of transaction amounts)\n",
    ").reset_index()\n",
    "\n",
    "# Display RFM DataFrame\n",
    "print(rfm_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JE-sqcoOhYs4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW0DcGeBioJ0"
   },
   "outputs": [],
   "source": [
    "duckdb.query('''\n",
    "    CREATE OR REPLACE TABLE rfm_scores AS\n",
    "\n",
    "    WITH rfm_feats AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            date,\n",
    "            DATEDIFF('days', MAX(date) OVER (), date) AS RECENCY,\n",
    "\n",
    "            -- Aggregating transaction counts as FREQUENCY\n",
    "            (\n",
    "                atm_transfer_in_count +\n",
    "                atm_transfer_out_count +\n",
    "                bank_transfer_in_count +\n",
    "                bank_transfer_out_count +\n",
    "                crypto_in_count +\n",
    "                crypto_out_count\n",
    "            ) AS FREQUENCY,\n",
    "\n",
    "            -- Summing monetary values as MONETARY\n",
    "            (\n",
    "                atm_transfer_in_volume +\n",
    "                atm_transfer_out_volume +\n",
    "                bank_transfer_in_volume +\n",
    "                bank_transfer_out_volume +\n",
    "                crypto_in_volume +\n",
    "                crypto_out_volume\n",
    "            ) AS MONETARY\n",
    "        FROM train_running_stats\n",
    "    ),\n",
    "    percent_ranks AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            date,\n",
    "            PERCENT_RANK() OVER(PARTITION BY date ORDER BY RECENCY) AS RECENCY_PERCENTILE,\n",
    "            PERCENT_RANK() OVER(PARTITION BY date ORDER BY FREQUENCY) AS FREQUENCY_PERCENTILE,\n",
    "            PERCENT_RANK() OVER(PARTITION BY date ORDER BY MONETARY) AS MONETARY_PERCENTILE\n",
    "        FROM rfm_feats\n",
    "    ),\n",
    "    rfm_feat_scores AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            date,\n",
    "\n",
    "            -- Assigning RECENCY scores\n",
    "            CASE\n",
    "                WHEN RECENCY_PERCENTILE <= 0.05 THEN 3\n",
    "                WHEN RECENCY_PERCENTILE <= 0.50 THEN 2\n",
    "                WHEN RECENCY_PERCENTILE <= 0.95 THEN 1\n",
    "                ELSE 0\n",
    "            END AS RECENCY_SCORE,\n",
    "\n",
    "            -- Assigning FREQUENCY scores\n",
    "            CASE\n",
    "                WHEN FREQUENCY_PERCENTILE <= 0.05 THEN 0\n",
    "                WHEN FREQUENCY_PERCENTILE <= 0.50 THEN 1\n",
    "                WHEN FREQUENCY_PERCENTILE <= 0.95 THEN 2\n",
    "                ELSE 3\n",
    "            END AS FREQUENCY_SCORE,\n",
    "\n",
    "            -- Assigning MONETARY scores\n",
    "            CASE\n",
    "                WHEN MONETARY_PERCENTILE <= 0.05 THEN 0\n",
    "                WHEN MONETARY_PERCENTILE <= 0.50 THEN 1\n",
    "                WHEN MONETARY_PERCENTILE <= 0.95 THEN 2\n",
    "                ELSE 3\n",
    "            END AS MONETARY_SCORE\n",
    "        FROM percent_ranks\n",
    "    )\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        date,\n",
    "        RECENCY_SCORE,\n",
    "        FREQUENCY_SCORE,\n",
    "        MONETARY_SCORE,\n",
    "        (\n",
    "            RECENCY_SCORE +\n",
    "            FREQUENCY_SCORE +\n",
    "            MONETARY_SCORE\n",
    "        ) AS TOTAL_RFM_SCORE\n",
    "    FROM rfm_feat_scores\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOEIDutIiohz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZkJTTtQionz"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import duckdb\n",
    "\n",
    "# Query to calculate RFM scores\n",
    "query = '''\n",
    "    CREATE OR REPLACE TABLE rfm_scores AS\n",
    "\n",
    "    WITH base_features AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            MAX(date) OVER () AS reference_date, -- Latest date in the dataset\n",
    "            date,\n",
    "\n",
    "            -- RECENCY: Days since the most recent transaction\n",
    "            DATEDIFF('days', date, MAX(date) OVER (PARTITION BY customer_id)) AS RECENCY,\n",
    "\n",
    "            -- FREQUENCY: Total number of transactions aggregated across all types\n",
    "            (\n",
    "                COALESCE(atm_transfer_in_count, 0) +\n",
    "                COALESCE(atm_transfer_out_count, 0) +\n",
    "                COALESCE(bank_transfer_in_count, 0) +\n",
    "                COALESCE(bank_transfer_out_count, 0) +\n",
    "                COALESCE(crypto_in_count, 0) +\n",
    "                COALESCE(crypto_out_count, 0)\n",
    "            ) AS FREQUENCY,\n",
    "\n",
    "            -- MONETARY: Sum of transaction volumes across all types\n",
    "            (\n",
    "                COALESCE(atm_transfer_in_volume, 0) +\n",
    "                COALESCE(atm_transfer_out_volume, 0) +\n",
    "                COALESCE(bank_transfer_in_volume, 0) +\n",
    "                COALESCE(bank_transfer_out_volume, 0) +\n",
    "                COALESCE(crypto_in_volume, 0) +\n",
    "                COALESCE(crypto_out_volume, 0)\n",
    "            ) AS MONETARY\n",
    "        FROM train_running_stats\n",
    "    ),\n",
    "    normalized_features AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            RECENCY,\n",
    "            FREQUENCY,\n",
    "            MONETARY,\n",
    "\n",
    "            -- Normalize each feature using min-max scaling for better scoring\n",
    "            (RECENCY - MIN(RECENCY) OVER ()) / (MAX(RECENCY) OVER () - MIN(RECENCY) OVER ()) AS RECENCY_NORM,\n",
    "            (FREQUENCY - MIN(FREQUENCY) OVER ()) / (MAX(FREQUENCY) OVER () - MIN(FREQUENCY) OVER ()) AS FREQUENCY_NORM,\n",
    "            (MONETARY - MIN(MONETARY) OVER ()) / (MAX(MONETARY) OVER () - MIN(MONETARY) OVER ()) AS MONETARY_NORM\n",
    "        FROM base_features\n",
    "    ),\n",
    "    rfm_scores_calculated AS (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "\n",
    "            -- Convert normalized values into categorical scores (3 = High, 0 = Low)\n",
    "            CASE\n",
    "                WHEN RECENCY_NORM <= 0.2 THEN 3\n",
    "                WHEN RECENCY_NORM <= 0.5 THEN 2\n",
    "                WHEN RECENCY_NORM <= 0.8 THEN 1\n",
    "                ELSE 0\n",
    "            END AS RECENCY_SCORE,\n",
    "\n",
    "            CASE\n",
    "                WHEN FREQUENCY_NORM <= 0.2 THEN 0\n",
    "                WHEN FREQUENCY_NORM <= 0.5 THEN 1\n",
    "                WHEN FREQUENCY_NORM <= 0.8 THEN 2\n",
    "                ELSE 3\n",
    "            END AS FREQUENCY_SCORE,\n",
    "\n",
    "            CASE\n",
    "                WHEN MONETARY_NORM <= 0.2 THEN 0\n",
    "                WHEN MONETARY_NORM <= 0.5 THEN 1\n",
    "                WHEN MONETARY_NORM <= 0.8 THEN 2\n",
    "                ELSE 3\n",
    "            END AS MONETARY_SCORE\n",
    "        FROM normalized_features\n",
    "    )\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        RECENCY_SCORE,\n",
    "        FREQUENCY_SCORE,\n",
    "        MONETARY_SCORE,\n",
    "        -- Total RFM score\n",
    "        (RECENCY_SCORE + FREQUENCY_SCORE + MONETARY_SCORE) AS TOTAL_RFM_SCORE\n",
    "    FROM rfm_scores_calculated\n",
    "'''\n",
    "\n",
    "# Execute the query in DuckDB\n",
    "duckdb.query(query)\n",
    "\n",
    "# Fetch and display RFM scores as a DataFrame (optional)\n",
    "rfm_scores = duckdb.query('SELECT * FROM rfm_scores').to_df()\n",
    "print(rfm_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QbLVLKLiosO"
   },
   "outputs": [],
   "source": [
    "# Disable progress bar for cleaner output\n",
    "duckdb.query('PRAGMA disable_progress_bar;')\n",
    "\n",
    "# Base path to your Parquet files\n",
    "BASEPATH = '/kaggle/input/neo-bank-non-sub-churn-prediction/'\n",
    "\n",
    "# Paths for train and test data\n",
    "train_path = BASEPATH + 'train_*.parquet'\n",
    "test_path = BASEPATH + 'test.parquet'\n",
    "\n",
    "# Load train data with advanced transformations\n",
    "duckdb.query(f'''\n",
    "    CREATE OR REPLACE VIEW train_data AS\n",
    "    SELECT\n",
    "        *,\n",
    "        DATE_PART('year', date) AS year,   -- Extract year from date column\n",
    "        DATE_PART('month', date) AS month, -- Extract month from date column\n",
    "        CASE\n",
    "            WHEN bank_transfer_in_volume + bank_transfer_out_volume > 10000 THEN 'High Value'\n",
    "            WHEN bank_transfer_in_volume + bank_transfer_out_volume > 5000 THEN 'Medium Value'\n",
    "            ELSE 'Low Value'\n",
    "        END AS value_category, -- Categorize based on transaction volume\n",
    "        REGEXP_EXTRACT(filename, r'train_(\\\\d{{4}})', 1) AS file_year -- Extract year from filename\n",
    "    FROM read_parquet('{train_path}', union_by_name=True, filename=True)\n",
    "''')\n",
    "\n",
    "# Load test data with selective column exclusion and transformations\n",
    "duckdb.query(f'''\n",
    "    CREATE OR REPLACE VIEW test_data AS\n",
    "    SELECT\n",
    "        *,\n",
    "        LENGTH(CAST(Usage AS VARCHAR)) AS usage_length, -- Compute length of Usage for advanced analysis\n",
    "        IFNULL(atm_transfer_in_volume, 0) AS atm_transfer_in_filled, -- Fill missing values for ATM transfer\n",
    "        CASE\n",
    "            WHEN crypto_in_volume + crypto_out_volume > 0 THEN 'Active in Crypto'\n",
    "            ELSE 'Inactive in Crypto'\n",
    "        END AS crypto_activity -- Add crypto activity indicator\n",
    "    FROM read_parquet('{test_path}', union_by_name=True, filename=True)\n",
    "''')\n",
    "\n",
    "# Verify train data schema\n",
    "train_schema = duckdb.query('DESCRIBE train_data').df()\n",
    "print(\"Train Data Schema:\")\n",
    "print(train_schema)\n",
    "\n",
    "# Verify test data schema\n",
    "test_schema = duckdb.query('DESCRIBE test_data').df()\n",
    "print(\"\\nTest Data Schema:\")\n",
    "print(test_schema)\n",
    "\n",
    "# Example preview of data\n",
    "train_preview = duckdb.query('SELECT * FROM train_data LIMIT 5').df()\n",
    "test_preview = duckdb.query('SELECT * FROM test_data LIMIT 5').df()\n",
    "\n",
    "print(\"\\nTrain Data Preview:\")\n",
    "print(train_preview)\n",
    "\n",
    "print(\"\\nTest Data Preview:\")\n",
    "print(test_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arel8V5jp8XY"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "\n",
    "# Path where Parquet files are uploaded in Colab\n",
    "data_path = '/content/'\n",
    "\n",
    "# Get the list of all Parquet files for training\n",
    "train_files = glob.glob(data_path + 'train_*.parquet')\n",
    "test_file = data_path + 'test.parquet'\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Create a view for train data by reading multiple Parquet files\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW train_data AS\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER () AS row_id,  -- Unique identifier for debugging or sorting\n",
    "        REPLACE(SPLIT_PART(filename, '/', -1), '.parquet', '') AS source_file -- File identifier\n",
    "    FROM read_parquet({','.join([f\"'{file}'\" for file in train_files])},\n",
    "                      union_by_name=True,\n",
    "                      filename=True)\n",
    "\"\"\")\n",
    "\n",
    "# Create a view for test data!pwd\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW test_data AS\n",
    "    SELECT\n",
    "        * EXCLUDE (Usage)  -- Modify column exclusion as needed\n",
    "    FROM read_parquet('{test_file}', union_by_name=True, filename=True)\n",
    "\"\"\")\n",
    "\n",
    "# Display schema and sample rows for validation\n",
    "print(\"Train Data Schema:\\n\")\n",
    "print(con.execute(\"DESCRIBE train_data\").fetchall())\n",
    "\n",
    "print(\"\\nTest Data Schema:\\n\")\n",
    "print(con.execute(\"DESCRIBE test_data\").fetchall())\n",
    "\n",
    "# Optionally preview the data\n",
    "print(\"\\nTrain Data Sample:\\n\")\n",
    "print(con.execute(\"SELECT * FROM train_data LIMIT 5\").fetchdf())\n",
    "\n",
    "\n",
    "print(\"\\nTest Data Sample:\\n\")\n",
    "print(con.execute(\"SELECT * FROM test_data LIMIT 5\").fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sHi9b2Ar4hp"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktvINiW9sPcC"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "\n",
    "# Path where Parquet files are uploaded in Colab\n",
    "data_path = '/content/'\n",
    "\n",
    "# Get all Parquet files for training using a dynamic wildcard\n",
    "train_files_pattern = f\"{data_path}train_*.parquet\"\n",
    "test_file = f\"{data_path}test.parquet\"\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Create a view for train data using the wildcard pattern\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW train_data AS\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER () AS row_id,  -- Unique identifier for debugging or sorting\n",
    "        REPLACE(SPLIT_PART(filename, '/', -1), '.parquet', '') AS source_file -- File identifier\n",
    "    FROM read_parquet('{train_files_pattern}',\n",
    "                      union_by_name=True,\n",
    "                      filename=True)\n",
    "\"\"\")\n",
    "\n",
    "# Create a view for test data\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW test_data AS\n",
    "    SELECT\n",
    "        * EXCLUDE (Usage)  -- Modify column exclusion as needed\n",
    "    FROM read_parquet('{test_file}',\n",
    "                      union_by_name=True,\n",
    "                      filename=True)\n",
    "\"\"\")\n",
    "\n",
    "# Display schema and sample rows for validation\n",
    "print(\"Train Data Schema:\\n\")\n",
    "print(con.execute(\"DESCRIBE train_data\").fetchall())\n",
    "\n",
    "print(\"\\nTest Data Schema:\\n\")\n",
    "print(con.execute(\"DESCRIBE test_data\").fetchall())\n",
    "\n",
    "# Optionally preview the data\n",
    "print(\"\\nTrain Data Sample:\\n\")\n",
    "print(con.execute(\"SELECT * FROM train_data LIMIT 5\").fetchdf())\n",
    "\n",
    "print(\"\\nTest Data Sample:\\n\")\n",
    "print(con.execute(\"SELECT * FROM test_data LIMIT 5\").fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrnJZ80esneN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjbJzoggtSU9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfnGCW7ytrNh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLAGKEJQuCNO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uWDF4_SupjR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nwEhFXSvoHU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
